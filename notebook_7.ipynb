{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supvervised Classification, Data Processing Pipelines\n",
    "\n",
    "**Advanced Astroinformatics Student Project**\n",
    "\n",
    "*N. Hernitschek, 2022*\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "## Contents\n",
    "* [Recap, Questions](#first-bullet)\n",
    "* [Supervised Classification - Binary Classifiers](#second-bullet)\n",
    "* [Supervised Classification - Multiclass Classifiers](#third-bullet)\n",
    "* [Data Processing Pipelines](#second-bullet)\n",
    "* [Summary](#fifth-bullet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recap, Questions <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "\n",
    "Time for questions!\n",
    "\n",
    "Your **tasks until this week** were:\n",
    "\n",
    "Use a the k-means algorithm on the three TESS feature data sets, including making diagnostic plots.\n",
    "\n",
    "Try to interpret your results.\n",
    "How do your results differ from the a) _TESS_lightcurves_outliercleaned, b) _TESS_lightcurves_median_after_detrended, c) _TESS_lightcurves_raw?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Supervised Classification - Binary Classifiers <a class=\"anchor\" id=\"third-bullet\"></a>\n",
    "\n",
    "Random forest classifiers generate decision trees from bootstrap samples. A interesting aspect of random forests is that the features on which to generate the tree are selected at random from the full set of features in the data (the number of features selected per split level is typically the square root of the total number of attributes). The final classification from the random forest is based on the averaging of the classifications of each of the individual decision trees. So, you can literally give it anything (including attributes that you might not otherwise think would be useful for classification).\n",
    "\n",
    "Random forests help to overcome some of the limitations of decision trees.\n",
    "\n",
    "As before, cross-validation can be used to determine the optimal depth. Generally the number of trees that are chosen is the number at which the cross-validation error plateaus.\n",
    "\n",
    "As there are a variety of supervised classification algorithms, in the following we apply a few to a test data set in order to distinguish quasars from stars in a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/anaconda3/envs/py310/lib/python3.10/site-packages/astroML/linear_model/linear_regression_errors.py:10: UserWarning: LinearRegressionwithErrors requires PyMC3 to be installed\n",
      "  warnings.warn('LinearRegressionwithErrors requires PyMC3 to be installed')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading DR7 quasar dataset from http://das.sdss.org/va/qsocat/dr7qso.dat.gz to /home/jorge/astroML_data\n",
      "Downloading http://das.sdss.org/va/qsocat/dr7qso.dat.gz\n",
      "[=========================================]  12.82Mb / 12.82Mb   \n",
      "GaussianNB\n",
      "LinearDiscriminantAnalysis\n",
      "QuadraticDiscriminantAnalysis\n",
      "LogisticRegression\n",
      "KNeighborsClassifier\n",
      "DecisionTreeClassifier\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to process string with tex because latex could not be found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/matplotlib/texmanager.py:233\u001b[0m, in \u001b[0;36mTexManager._run_checked_subprocess\u001b[0;34m(self, command, tex, cwd)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 233\u001b[0m     report \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39;49mcheck_output(\n\u001b[1;32m    234\u001b[0m         command, cwd\u001b[39m=\u001b[39;49mcwd \u001b[39mif\u001b[39;49;00m cwd \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtexcache,\n\u001b[1;32m    235\u001b[0m         stderr\u001b[39m=\u001b[39;49msubprocess\u001b[39m.\u001b[39;49mSTDOUT)\n\u001b[1;32m    236\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/subprocess.py:420\u001b[0m, in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m empty\n\u001b[0;32m--> 420\u001b[0m \u001b[39mreturn\u001b[39;00m run(\u001b[39m*\u001b[39;49mpopenargs, stdout\u001b[39m=\u001b[39;49mPIPE, timeout\u001b[39m=\u001b[39;49mtimeout, check\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    421\u001b[0m            \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39mstdout\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/subprocess.py:501\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mstderr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m PIPE\n\u001b[0;32m--> 501\u001b[0m \u001b[39mwith\u001b[39;00m Popen(\u001b[39m*\u001b[39;49mpopenargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mas\u001b[39;00m process:\n\u001b[1;32m    502\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/subprocess.py:966\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[1;32m    964\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m--> 966\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m    967\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m    968\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m    969\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m    970\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m    971\u001b[0m                         errread, errwrite,\n\u001b[1;32m    972\u001b[0m                         restore_signals,\n\u001b[1;32m    973\u001b[0m                         gid, gids, uid, umask,\n\u001b[1;32m    974\u001b[0m                         start_new_session)\n\u001b[1;32m    975\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    976\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/subprocess.py:1842\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1841\u001b[0m         err_msg \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1842\u001b[0m     \u001b[39mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1843\u001b[0m \u001b[39mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'latex'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/IPython/core/formatters.py:339\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 339\u001b[0m     \u001b[39mreturn\u001b[39;00m printer(obj)\n\u001b[1;32m    340\u001b[0m \u001b[39m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    341\u001b[0m method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/IPython/core/pylabtools.py:151\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend_bases\u001b[39;00m \u001b[39mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    149\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 151\u001b[0m fig\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mprint_figure(bytes_io, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    152\u001b[0m data \u001b[39m=\u001b[39m bytes_io\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m fmt \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msvg\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/matplotlib/backend_bases.py:2295\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2289\u001b[0m     renderer \u001b[39m=\u001b[39m _get_renderer(\n\u001b[1;32m   2290\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure,\n\u001b[1;32m   2291\u001b[0m         functools\u001b[39m.\u001b[39mpartial(\n\u001b[1;32m   2292\u001b[0m             print_method, orientation\u001b[39m=\u001b[39morientation)\n\u001b[1;32m   2293\u001b[0m     )\n\u001b[1;32m   2294\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mgetattr\u001b[39m(renderer, \u001b[39m\"\u001b[39m\u001b[39m_draw_disabled\u001b[39m\u001b[39m\"\u001b[39m, nullcontext)():\n\u001b[0;32m-> 2295\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m   2297\u001b[0m \u001b[39mif\u001b[39;00m bbox_inches:\n\u001b[1;32m   2298\u001b[0m     \u001b[39mif\u001b[39;00m bbox_inches \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtight\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/matplotlib/artist.py:73\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39m@wraps\u001b[39m(draw)\n\u001b[1;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw_wrapper\u001b[39m(artist, renderer, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 73\u001b[0m     result \u001b[39m=\u001b[39m draw(artist, renderer, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m renderer\u001b[39m.\u001b[39m_rasterizing:\n\u001b[1;32m     75\u001b[0m         renderer\u001b[39m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/matplotlib/artist.py:50\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     51\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/matplotlib/figure.py:2810\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2807\u001b[0m         \u001b[39m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   2809\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 2810\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[1;32m   2811\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[1;32m   2813\u001b[0m \u001b[39mfor\u001b[39;00m sfig \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubfigs:\n\u001b[1;32m   2814\u001b[0m     sfig\u001b[39m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m    133\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/matplotlib/artist.py:50\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     51\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/matplotlib/axes/_base.py:3082\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3079\u001b[0m         a\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[1;32m   3080\u001b[0m     renderer\u001b[39m.\u001b[39mstop_rasterizing()\n\u001b[0;32m-> 3082\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[1;32m   3083\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[1;32m   3085\u001b[0m renderer\u001b[39m.\u001b[39mclose_group(\u001b[39m'\u001b[39m\u001b[39maxes\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   3086\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstale \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m    133\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/matplotlib/artist.py:50\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     51\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/matplotlib/axis.py:1159\u001b[0m, in \u001b[0;36mAxis.draw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m renderer\u001b[39m.\u001b[39mopen_group(\u001b[39m__name__\u001b[39m, gid\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_gid())\n\u001b[1;32m   1158\u001b[0m ticks_to_draw \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_ticks()\n\u001b[0;32m-> 1159\u001b[0m ticklabelBoxes, ticklabelBoxes2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_tick_bboxes(ticks_to_draw,\n\u001b[1;32m   1160\u001b[0m                                                         renderer)\n\u001b[1;32m   1162\u001b[0m \u001b[39mfor\u001b[39;00m tick \u001b[39min\u001b[39;00m ticks_to_draw:\n\u001b[1;32m   1163\u001b[0m     tick\u001b[39m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/matplotlib/axis.py:1085\u001b[0m, in \u001b[0;36mAxis._get_tick_bboxes\u001b[0;34m(self, ticks, renderer)\u001b[0m\n\u001b[1;32m   1083\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_tick_bboxes\u001b[39m(\u001b[39mself\u001b[39m, ticks, renderer):\n\u001b[1;32m   1084\u001b[0m     \u001b[39m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[39mreturn\u001b[39;00m ([tick\u001b[39m.\u001b[39mlabel1\u001b[39m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   1086\u001b[0m              \u001b[39mfor\u001b[39;00m tick \u001b[39min\u001b[39;00m ticks \u001b[39mif\u001b[39;00m tick\u001b[39m.\u001b[39mlabel1\u001b[39m.\u001b[39mget_visible()],\n\u001b[1;32m   1087\u001b[0m             [tick\u001b[39m.\u001b[39mlabel2\u001b[39m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   1088\u001b[0m              \u001b[39mfor\u001b[39;00m tick \u001b[39min\u001b[39;00m ticks \u001b[39mif\u001b[39;00m tick\u001b[39m.\u001b[39mlabel2\u001b[39m.\u001b[39mget_visible()])\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/matplotlib/axis.py:1085\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1083\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_tick_bboxes\u001b[39m(\u001b[39mself\u001b[39m, ticks, renderer):\n\u001b[1;32m   1084\u001b[0m     \u001b[39m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[39mreturn\u001b[39;00m ([tick\u001b[39m.\u001b[39;49mlabel1\u001b[39m.\u001b[39;49mget_window_extent(renderer)\n\u001b[1;32m   1086\u001b[0m              \u001b[39mfor\u001b[39;00m tick \u001b[39min\u001b[39;00m ticks \u001b[39mif\u001b[39;00m tick\u001b[39m.\u001b[39mlabel1\u001b[39m.\u001b[39mget_visible()],\n\u001b[1;32m   1087\u001b[0m             [tick\u001b[39m.\u001b[39mlabel2\u001b[39m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   1088\u001b[0m              \u001b[39mfor\u001b[39;00m tick \u001b[39min\u001b[39;00m ticks \u001b[39mif\u001b[39;00m tick\u001b[39m.\u001b[39mlabel2\u001b[39m.\u001b[39mget_visible()])\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/matplotlib/text.py:910\u001b[0m, in \u001b[0;36mText.get_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCannot get window extent w/o renderer\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    909\u001b[0m \u001b[39mwith\u001b[39;00m cbook\u001b[39m.\u001b[39m_setattr_cm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure, dpi\u001b[39m=\u001b[39mdpi):\n\u001b[0;32m--> 910\u001b[0m     bbox, info, descent \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_layout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_renderer)\n\u001b[1;32m    911\u001b[0m     x, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_unitless_position()\n\u001b[1;32m    912\u001b[0m     x, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_transform()\u001b[39m.\u001b[39mtransform((x, y))\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/matplotlib/text.py:309\u001b[0m, in \u001b[0;36mText._get_layout\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    306\u001b[0m ys \u001b[39m=\u001b[39m []\n\u001b[1;32m    308\u001b[0m \u001b[39m# Full vertical extent of font, including ascenders and descenders:\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m _, lp_h, lp_d \u001b[39m=\u001b[39m renderer\u001b[39m.\u001b[39;49mget_text_width_height_descent(\n\u001b[1;32m    310\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mlp\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fontproperties,\n\u001b[1;32m    311\u001b[0m     ismath\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mTeX\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_usetex() \u001b[39melse\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    312\u001b[0m min_dy \u001b[39m=\u001b[39m (lp_h \u001b[39m-\u001b[39m lp_d) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_linespacing\n\u001b[1;32m    314\u001b[0m \u001b[39mfor\u001b[39;00m i, line \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(lines):\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:259\u001b[0m, in \u001b[0;36mRendererAgg.get_text_width_height_descent\u001b[0;34m(self, s, prop, ismath)\u001b[0m\n\u001b[1;32m    257\u001b[0m     texmanager \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_texmanager()\n\u001b[1;32m    258\u001b[0m     fontsize \u001b[39m=\u001b[39m prop\u001b[39m.\u001b[39mget_size_in_points()\n\u001b[0;32m--> 259\u001b[0m     w, h, d \u001b[39m=\u001b[39m texmanager\u001b[39m.\u001b[39;49mget_text_width_height_descent(\n\u001b[1;32m    260\u001b[0m         s, fontsize, renderer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    261\u001b[0m     \u001b[39mreturn\u001b[39;00m w, h, d\n\u001b[1;32m    263\u001b[0m \u001b[39mif\u001b[39;00m ismath:\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/matplotlib/texmanager.py:335\u001b[0m, in \u001b[0;36mTexManager.get_text_width_height_descent\u001b[0;34m(self, tex, fontsize, renderer)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m tex\u001b[39m.\u001b[39mstrip() \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    334\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[0;32m--> 335\u001b[0m dvifile \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_dvi(tex, fontsize)\n\u001b[1;32m    336\u001b[0m dpi_fraction \u001b[39m=\u001b[39m renderer\u001b[39m.\u001b[39mpoints_to_pixels(\u001b[39m1.\u001b[39m) \u001b[39mif\u001b[39;00m renderer \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m    337\u001b[0m \u001b[39mwith\u001b[39;00m dviread\u001b[39m.\u001b[39mDvi(dvifile, \u001b[39m72\u001b[39m \u001b[39m*\u001b[39m dpi_fraction) \u001b[39mas\u001b[39;00m dvi:\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/matplotlib/texmanager.py:271\u001b[0m, in \u001b[0;36mTexManager.make_dvi\u001b[0;34m(self, tex, fontsize)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[39m# Generate the dvi in a temporary directory to avoid race\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     \u001b[39m# conditions e.g. if multiple processes try to process the same tex\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[39m# string at the same time.  Having tmpdir be a subdirectory of the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[39m# the absolute path may contain characters (e.g. ~) that TeX does\u001b[39;00m\n\u001b[1;32m    269\u001b[0m     \u001b[39m# not support.)\u001b[39;00m\n\u001b[1;32m    270\u001b[0m     \u001b[39mwith\u001b[39;00m TemporaryDirectory(\u001b[39mdir\u001b[39m\u001b[39m=\u001b[39mPath(dvifile)\u001b[39m.\u001b[39mparent) \u001b[39mas\u001b[39;00m tmpdir:\n\u001b[0;32m--> 271\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_checked_subprocess(\n\u001b[1;32m    272\u001b[0m             [\u001b[39m\"\u001b[39;49m\u001b[39mlatex\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m-interaction=nonstopmode\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m--halt-on-error\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    273\u001b[0m              \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../\u001b[39;49m\u001b[39m{\u001b[39;49;00mtexfile\u001b[39m.\u001b[39;49mname\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m], tex, cwd\u001b[39m=\u001b[39;49mtmpdir)\n\u001b[1;32m    274\u001b[0m         (Path(tmpdir) \u001b[39m/\u001b[39m Path(dvifile)\u001b[39m.\u001b[39mname)\u001b[39m.\u001b[39mreplace(dvifile)\n\u001b[1;32m    275\u001b[0m \u001b[39mreturn\u001b[39;00m dvifile\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/matplotlib/texmanager.py:237\u001b[0m, in \u001b[0;36mTexManager._run_checked_subprocess\u001b[0;34m(self, command, tex, cwd)\u001b[0m\n\u001b[1;32m    233\u001b[0m     report \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39mcheck_output(\n\u001b[1;32m    234\u001b[0m         command, cwd\u001b[39m=\u001b[39mcwd \u001b[39mif\u001b[39;00m cwd \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtexcache,\n\u001b[1;32m    235\u001b[0m         stderr\u001b[39m=\u001b[39msubprocess\u001b[39m.\u001b[39mSTDOUT)\n\u001b[1;32m    236\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m--> 237\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    238\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mFailed to process string with tex because \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m could not be \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    239\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mfound\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(command[\u001b[39m0\u001b[39m])) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39mexcept\u001b[39;00m subprocess\u001b[39m.\u001b[39mCalledProcessError \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m{prog}\u001b[39;00m\u001b[39m was not able to process the following string:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    243\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m{tex!r}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m             tex\u001b[39m=\u001b[39mtex\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39municode_escape\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m    248\u001b[0m             exc\u001b[39m=\u001b[39mexc\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m))) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to process string with tex because latex could not be found"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from astroML.utils import split_samples\n",
    "from astroML.utils import completeness_contamination\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import (LinearDiscriminantAnalysis,\n",
    "                                           QuadraticDiscriminantAnalysis)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from astroML.classification import GMMBayes\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=10, usetex=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Fetch data and split into training and test samples\n",
    "from astroML.datasets import fetch_dr7_quasar\n",
    "from astroML.datasets import fetch_sdss_sspp\n",
    "\n",
    "quasars = fetch_dr7_quasar()\n",
    "stars = fetch_sdss_sspp()\n",
    "\n",
    "# Truncate data for speed\n",
    "quasars = quasars[::5]\n",
    "stars = stars[::5]\n",
    "\n",
    "# stack colors into matrix X\n",
    "Nqso = len(quasars)\n",
    "Nstars = len(stars)\n",
    "X = np.empty((Nqso + Nstars, 4), dtype=float)\n",
    "\n",
    "X[:Nqso, 0] = quasars['mag_u'] - quasars['mag_g']\n",
    "X[:Nqso, 1] = quasars['mag_g'] - quasars['mag_r']\n",
    "X[:Nqso, 2] = quasars['mag_r'] - quasars['mag_i']\n",
    "X[:Nqso, 3] = quasars['mag_i'] - quasars['mag_z']\n",
    "\n",
    "X[Nqso:, 0] = stars['upsf'] - stars['gpsf']\n",
    "X[Nqso:, 1] = stars['gpsf'] - stars['rpsf']\n",
    "X[Nqso:, 2] = stars['rpsf'] - stars['ipsf']\n",
    "X[Nqso:, 3] = stars['ipsf'] - stars['zpsf']\n",
    "\n",
    "y = np.zeros(Nqso + Nstars, dtype=int)\n",
    "y[:Nqso] = 1\n",
    "\n",
    "# split into training and test sets\n",
    "(X_train, X_test), (y_train, y_test) = split_samples(X, y, [0.9, 0.1],\n",
    "                                                     random_state=0)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute fits for all the classifiers\n",
    "def compute_results(*args):\n",
    "    names = []\n",
    "    probs = []\n",
    "\n",
    "    for classifier, kwargs in args:\n",
    "        print(classifier.__name__)\n",
    "        model = classifier(**kwargs)\n",
    "        model.fit(X, y)\n",
    "        y_prob = model.predict_proba(X_test)\n",
    "\n",
    "        names.append(classifier.__name__)\n",
    "        probs.append(y_prob[:, 1])\n",
    "\n",
    "    return names, probs\n",
    "\n",
    "LRclass_weight = dict([(i, np.sum(y_train == i)) for i in (0, 1)])\n",
    "\n",
    "names, probs = compute_results((GaussianNB, {}),\n",
    "                               (LinearDiscriminantAnalysis, {}),\n",
    "                               (QuadraticDiscriminantAnalysis, {}),\n",
    "                               (LogisticRegression,\n",
    "                                dict(class_weight=LRclass_weight)),\n",
    "                               (KNeighborsClassifier,\n",
    "                                dict(n_neighbors=10)),\n",
    "                               (DecisionTreeClassifier,\n",
    "                                dict(random_state=0, max_depth=12,\n",
    "                                     criterion='entropy')))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot results\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "fig.subplots_adjust(left=0.1, right=0.95, bottom=0.15, top=0.9, wspace=0.25)\n",
    "\n",
    "# First axis shows the data\n",
    "ax1 = fig.add_subplot(131)\n",
    "#im = ax1.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=4,\n",
    "#                 linewidths=0, edgecolors='none',\n",
    "#                 cmap=plt.cm.binary)\n",
    "#ax1.legend ('quasars', 'stars')\n",
    "\n",
    "\n",
    "ax1.scatter(X_test[:, 0][y_test==0], X_test[:, 1][y_test==0], c='grey', s=4,\n",
    "                 linewidths=0, edgecolors='none', label='stars')\n",
    "ax1.scatter(X_test[:, 0][y_test==1], X_test[:, 1][y_test==1], c='k', s=4,\n",
    "                 linewidths=0, edgecolors='none',label='quasars')\n",
    "\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax1.set_xlim(-0.5, 3.0)\n",
    "ax1.set_ylim(-0.3, 1.4)\n",
    "ax1.set_xlabel('$u - g$')\n",
    "ax1.set_ylabel('$g - r$')\n",
    "\n",
    "labels = dict(GaussianNB='GNB',\n",
    "              LinearDiscriminantAnalysis='LDA',\n",
    "              QuadraticDiscriminantAnalysis='QDA',\n",
    "              KNeighborsClassifier='KNN',\n",
    "              DecisionTreeClassifier='DT',\n",
    "              LogisticRegression='LR')\n",
    "\n",
    "# Second axis shows the ROC curves\n",
    "ax2 = fig.add_subplot(132)\n",
    "for name, y_prob in zip(names, probs):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "\n",
    "    fpr = np.concatenate([[0], fpr])\n",
    "    tpr = np.concatenate([[0], tpr])\n",
    "    \n",
    "    ax2.plot(fpr, tpr, label=labels[name])\n",
    "    \n",
    "    ######\n",
    "    \n",
    "# Third axis shows the completeness-efficiency curves\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "for name, y_prob in zip(names, probs):\n",
    "    comp = np.zeros_like(thresholds)\n",
    "    cont = np.zeros_like(thresholds)\n",
    "    for i, t in enumerate(thresholds):\n",
    "        y_pred = (y_prob >= t)\n",
    "        comp[i], cont[i] = completeness_contamination(y_pred, y_test)\n",
    "    ax3.plot(comp,1 - cont, label=labels[name])\n",
    "    \n",
    "\n",
    "ax2.legend(loc=4)\n",
    "ax2.set_title('ROC curves')\n",
    "ax2.set_xlabel('false positive rate')\n",
    "ax2.set_ylabel('true positive rate')\n",
    "ax2.set_xlim(0, 0.15)\n",
    "ax2.set_ylim(0.6, 1.01)\n",
    "ax2.xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "\n",
    "\n",
    "ax3.set_title('completeness-efficiency curves')\n",
    "ax3.set_xlabel('efficiency')\n",
    "ax3.set_ylabel('completeness')\n",
    "ax3.set_xlim(0, 1.0)\n",
    "ax3.set_ylim(0.2, 1.02)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see two **diagnostic plots** (see also lecture slides 5).\n",
    "\n",
    "A receiver operating characteristic curve, or **ROC curve**, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. \n",
    "\n",
    "\n",
    "One concern about ROC curves is that they are sensitive to the relative\n",
    "sample sizes: if there are many more background events than source\n",
    "events, small false positive results can dominate a signal.\n",
    "For these cases we can plot completeness versus efficiency.\n",
    "\n",
    "\n",
    "\n",
    "Here we see that to get higher completeness, you could actually suffer significantly in terms of efficiency, but your false positive rate (FPR) might not go up that much if there are lots of true negatives.\n",
    "Note that the desired completeness and efficiency is chosen by selecting a decision boundary. The curves show what these possible choices are. Generally, one wants to chose a decision boundary that **maximizes the area\n",
    "under the** ROC (or completeness versus efficiency) **curve**.\n",
    "\n",
    "\n",
    "remember: we defined\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "{\\rm completeness} = \\frac{\\rm true\\ positives}{\\rm true\\ positives + false\\ negatives}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*} \n",
    "{\\rm contamination} = \\frac{\\rm false\\ positives}{\\rm true\\ positives + false\\ positives} = {\\rm false\\ discovery\\ rate}\n",
    "\\end{equation*}\n",
    "\n",
    "Instead of contamination, often also efficiency (also called purity) is used: ${\\rm efficiency} = \\rm{(1 - contamination)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Supervised Classification - Multiclass Classifiers <a class=\"anchor\" id=\"third-bullet\"></a>\n",
    "\n",
    "\n",
    "In many cases we not only want to discriminate between two classes, but many. This is called a **multiclass classifier**. That's exactly what we need for the TESS data.\n",
    "In the following, you will see a complete example using the `iris` test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.datasets import load_iris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the data set from the original iris data set to look more like a typical astronomical data set\n",
    "iris = load_iris(as_frame=True)\n",
    "\n",
    "data = iris.data\n",
    "target = iris.target\n",
    "\n",
    "#replace target with names like usually found in astronomical data sets\n",
    "target_names = target.replace([0, 1, 2], ['setosa', 'versicolor', 'virginica'])\n",
    "\n",
    "dataset = pd.concat([data,target_names.reindex(data.index)], axis=1)\n",
    "\n",
    "\n",
    "#Renaming the columns\n",
    "dataset.columns = ['sepal length in cm', 'sepal width in cm','petal length in cm','petal width in cm','species']\n",
    "print('Shape of the dataset: ' + str(data.shape))\n",
    "\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "\n",
    "# we now have our data set, like we have read it from a cvs text file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the dependent variable class**\n",
    "\n",
    "We are converting species column values from ['Iris-setosa','Iris-versicolor','Iris-virginica'] to [0,1,2]. This is an essential step as the scikit-learn's Random Forest can't predict text — it can only predict numbers.\n",
    "\n",
    "Yes, we had just converted it from numbers to text labels... This was done to mimic we were reading in a typical text file.\n",
    "\n",
    "In addition, we need to store the factor conversions to remember what number is substituting the text.\n",
    "The code below will perform the following:\n",
    "\n",
    " *   Use `pandas` factorize function to factorize the species column in the dataset. This will create both factors and the definitions for the factors.\n",
    " *   Store the factorized column as species.\n",
    " *   Store the definitions for the factors.\n",
    " *   Show the first five rows for the species column and the defintions array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the dependent variable class\n",
    "factor = pd.factorize(dataset['species'])\n",
    "dataset.species = factor[0]\n",
    "definitions = factor[1]\n",
    "print(dataset.species.head())\n",
    "print(definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into independent and dependent variables\n",
    "X = dataset.iloc[:,0:4].values\n",
    "y = dataset.iloc[:,4].values\n",
    "print('The independent features set: ')\n",
    "print(X[:5,:])\n",
    "print('The dependent variable: ')\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train-Test Data Splitting**\n",
    "\n",
    "We will use 75% of the data for training and the remaining 25% as test data (i.e., 75% of 150 rows as 112 rows for training and 38 rows for testing). \n",
    "\n",
    "Also, the reason for such high number of test case percentages is due to fewer numbers of rows for the model. Generally, 80/20 rule for train-test is used when data is sufficiently high.\n",
    "\n",
    "The below code uses the prebuilt function 'train_test_split' in a sklearn library for creating the train and test arrays for both independent and dependent variable. Also, random_state = 21 is assigned for random distribution of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Training and Test set from data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 21)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Scaling**\n",
    "\n",
    "This is a very important step in machine learning. It helps the algorithm quickly learn a better solution to the problem.\n",
    "\n",
    "We will use a standard scaler provided in the sklearn library. It subtracts the mean value of the observation and then divides it by the unit variance of the observation.\n",
    "We will perform the following steps:\n",
    "\n",
    "    Define a scaler by calling the function from sklearn library.\n",
    "    Transform train feature dataset (X_train) and fit the scaler on train feature dataset.\n",
    "    Use the scaler to transform test feature dataset (X_test).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the model**\n",
    "\n",
    "We define the parameters for the random forest training as follows:\n",
    "\n",
    "    n_estimators: This is the number of trees in the random forest classification. We have defined 10 trees in our random forest.\n",
    "    criterion: This is the loss function used to measure the quality of the split. There are two available options in sklearn — gini and entropy. We have used entropy.\n",
    "    random_state: This is the seed used by the random state generator for randomizing the dataset.\n",
    "\n",
    "Next, we use the training dataset (both dependent and independent to train the random forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Random Forest Classification to the Training set\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 42)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating the performance**\n",
    "\n",
    "Performance evaluation of the trained model consists of following steps:\n",
    "\n",
    "    Predicting the species class of the test data using test feature set (X_test). We will use the predict function of the random forest classifier to predict classes.\n",
    "    Converting the numeric classes of the predicted values and the test actual values into textual equivalent. This involves the following steps:\n",
    "        Creating dictionary for mapping tables from class to text — we use dict function along with zip to create the required dictionary.\n",
    "        Transforming the test-actual and test-predict database from numeric classes to textual classes.\n",
    "        Evaluating the performance of the classifier using Confusion Matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "#Reverse factorize (converting y_pred from 0s,1s and 2s to Iris-setosa, Iris-versicolor and Iris-virginica\n",
    "reversefactor = dict(zip(range(3),definitions))\n",
    "y_test = np.vectorize(reversefactor.get)(y_test)\n",
    "y_pred = np.vectorize(reversefactor.get)(y_pred)\n",
    "# Making the Confusion Matrix\n",
    "print(pd.crosstab(y_test, y_pred, rownames=['Actual Species'], colnames=['Predicted Species']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(zip(dataset.columns[0:4], classifier.feature_importances_)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 10-fold verification <a class=\"anchor\" id=\"third-bullet\"></a>\n",
    "\n",
    "\n",
    "In the example above we saw how to split the initial data set into a training and testing data set.\n",
    "Whereas this can be done as a first step, it is highly recommended using **10-fold verification**, where in turn 10 % of the data are held out as a test set and 90 % are used for training.\n",
    "\n",
    "In the following, we will implement this in the same way as the example above, using the `iris`data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (150, 4)\n",
      "     sepal length in cm  sepal width in cm  petal length in cm  \\\n",
      "0                   5.1                3.5                 1.4   \n",
      "1                   4.9                3.0                 1.4   \n",
      "2                   4.7                3.2                 1.3   \n",
      "3                   4.6                3.1                 1.5   \n",
      "4                   5.0                3.6                 1.4   \n",
      "..                  ...                ...                 ...   \n",
      "145                 6.7                3.0                 5.2   \n",
      "146                 6.3                2.5                 5.0   \n",
      "147                 6.5                3.0                 5.2   \n",
      "148                 6.2                3.4                 5.4   \n",
      "149                 5.9                3.0                 5.1   \n",
      "\n",
      "     petal width in cm    species  \n",
      "0                  0.2     setosa  \n",
      "1                  0.2     setosa  \n",
      "2                  0.2     setosa  \n",
      "3                  0.2     setosa  \n",
      "4                  0.2     setosa  \n",
      "..                 ...        ...  \n",
      "145                2.3  virginica  \n",
      "146                1.9  virginica  \n",
      "147                2.0  virginica  \n",
      "148                2.3  virginica  \n",
      "149                1.8  virginica  \n",
      "\n",
      "[150 rows x 5 columns]\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: species, dtype: int64\n",
      "Index(['setosa', 'versicolor', 'virginica'], dtype='object')\n",
      "The independent features set: \n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "The dependent variable: \n",
      "[0 0 0 0 0]\n",
      "X size:  600\n",
      "test_x size:  60\n",
      "X size:  600\n",
      "test_x size:  60\n",
      "X size:  600\n",
      "test_x size:  60\n",
      "X size:  600\n",
      "test_x size:  60\n",
      "X size:  600\n",
      "test_x size:  60\n",
      "X size:  600\n",
      "test_x size:  60\n",
      "X size:  600\n",
      "test_x size:  60\n",
      "X size:  600\n",
      "test_x size:  60\n",
      "X size:  600\n",
      "test_x size:  60\n",
      "X size:  600\n",
      "test_x size:  60\n",
      "actual\n",
      "[0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 1. 1. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 2. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 1. 2. 2. 2. 2. 2. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 2. 2. 2. 2. 2. 2.]\n",
      "predicted\n",
      "[0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 0. 0. 0. 0. 0. 0. 1. 1. 2.\n",
      " 1. 1. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 2. 2. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 1. 1. 1. 2. 2. 2. 2.\n",
      " 1. 1. 2. 0. 0. 0. 0. 0. 0. 1. 2. 1. 1. 2. 2. 2. 2. 2. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 2. 1. 1. 2. 2. 2. 2. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 2. 1. 1. 1. 2. 2.]\n",
      "input_folds\n",
      "[array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2])\n",
      " array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2])\n",
      " array([0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2])\n",
      " array([0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2])\n",
      " array([0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2])\n",
      " array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2])\n",
      " array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2])\n",
      " array([0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2])\n",
      " array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2])\n",
      " array([0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2])]\n",
      "result_folds\n",
      "[array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2])\n",
      " array([0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 2, 2])\n",
      " array([0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 2])\n",
      " array([0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2])\n",
      " array([0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2])\n",
      " array([0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 2, 2, 2, 2, 2])\n",
      " array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2])\n",
      " array([0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 2, 2, 2, 2])\n",
      " array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2])\n",
      " array([0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2])]\n",
      "[[50  0  0]\n",
      " [ 0 47  3]\n",
      " [ 0  7 43]]\n",
      "TPR\n",
      "[1.   0.94 0.86]\n",
      "FPR\n",
      "[0.   0.07 0.03]\n",
      "ACC\n",
      "[1.         0.93333333 0.93333333]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Importing Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "#create the data set from the original iris data set to look more like a typical astronomical data set\n",
    "iris = load_iris(as_frame=True)\n",
    "\n",
    "data = iris.data\n",
    "target = iris.target\n",
    "\n",
    "#replace target with names like usually found in astronomical data sets\n",
    "target_names = target.replace([0, 1, 2], ['setosa', 'versicolor', 'virginica'])\n",
    "\n",
    "dataset = pd.concat([data,target_names.reindex(data.index)], axis=1)\n",
    "\n",
    "\n",
    "#Renaming the columns\n",
    "dataset.columns = ['sepal length in cm', 'sepal width in cm','petal length in cm','petal width in cm','species']\n",
    "print('Shape of the dataset: ' + str(data.shape))\n",
    "\n",
    "\n",
    "# we now have our data set, like we have read it from a cvs text file\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "\n",
    "#Creating the dependent variable class\n",
    "factor = pd.factorize(dataset['species'])\n",
    "dataset.species = factor[0]\n",
    "definitions = factor[1]\n",
    "print(dataset.species.head())\n",
    "print(definitions)\n",
    "\n",
    "#Splitting the data into independent and dependent variables\n",
    "X = dataset.iloc[:,0:4].values\n",
    "y = dataset.iloc[:,4].values\n",
    "print('The independent features set: ')\n",
    "print(X[:5,:])\n",
    "print('The dependent variable: ')\n",
    "print(y[:5])\n",
    "\n",
    "\n",
    "folds = 10\n",
    "k_fold = KFold(folds, shuffle=True, random_state=1)\n",
    "\n",
    "predicted_targets = np.array([])\n",
    "actual_targets = np.array([])\n",
    "\n",
    "\n",
    "input_folds = np.empty(folds, dtype=object)\n",
    "result_folds = np.empty(folds, dtype=object)\n",
    "\n",
    "fold=0\n",
    "\n",
    "for train_ix, test_ix in k_fold.split(X):\n",
    "        train_x, train_y, test_x, test_y = X[train_ix], y[train_ix], X[test_ix], y[test_ix]\n",
    "        \n",
    "        \n",
    "        print('X size: ', X.size)\n",
    "        print('test_x size: ', test_x.size)\n",
    "\n",
    "        # Fit the classifier\n",
    "       # classifier = svm.SVC().fit(train_x, train_y)\n",
    "    \n",
    "        # Feature Scaling\n",
    "        scaler = StandardScaler()\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        test_x = scaler.transform(test_x)\n",
    "    \n",
    "        classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 42)\n",
    "        classifier.fit(train_x, train_y)\n",
    "\n",
    "        # Predict the labels of the test set samples\n",
    "        predicted_labels = classifier.predict(test_x)\n",
    "\n",
    "        predicted_targets = np.append(predicted_targets, predicted_labels)\n",
    "        actual_targets = np.append(actual_targets, test_y)\n",
    "        \n",
    "        \n",
    "        # instead, save predicted, actual in 2d matrixpredite\n",
    "        input_folds[fold] = test_y\n",
    "        result_folds[fold] = predicted_labels\n",
    "        \n",
    "        fold=fold+1\n",
    "        \n",
    "\n",
    "print('actual')        \n",
    "print(actual_targets)  \n",
    "print('predicted')        \n",
    "print(predicted_targets)       \n",
    "\n",
    "print('input_folds')        \n",
    "print(input_folds)\n",
    "print('result_folds')        \n",
    "print(result_folds)\n",
    "        \n",
    "        \n",
    "cnf_matrix = confusion_matrix(actual_targets, predicted_targets)\n",
    "\n",
    "\n",
    "print(cnf_matrix)\n",
    "\n",
    "\n",
    "\n",
    "# here we calculate the scores for all three classes to report the classifier performance\n",
    "\n",
    "FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
    "FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "TP = np.diag(cnf_matrix)\n",
    "TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy for each class\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "print('TPR')\n",
    "print(TPR)\n",
    "\n",
    "print('FPR')\n",
    "print(FPR)\n",
    "\n",
    "print('ACC')\n",
    "print(ACC)\n",
    "\n",
    "\n",
    "# reuse plotting code from notebook 5\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def to_density(cf):\n",
    "  '''\n",
    "  This function will take in a confusion matrix cf and return the relative 'density' of every element in each row.\n",
    "  ---------\n",
    "  cf: Confusion matrix to be passed in\n",
    "  '''\n",
    "  density = []\n",
    "  n, k = cf.shape\n",
    "  for i in range(n):\n",
    "    density_row = []\n",
    "    for j in range(k):\n",
    "      total_stars = sum(cf[i])\n",
    "      density_row.append(cf[i][j]/total_stars)\n",
    "    density.append(density_row)\n",
    "  return np.array(density)\n",
    "\n",
    "\n",
    "def make_confusion_matrix(cf_,\n",
    "                          xlabel, ylabel,\n",
    "                          group_names=None,\n",
    "                          categories_x='auto',\n",
    "                          categories_y='auto',\n",
    "                          count=True,\n",
    "                          cbar=True,\n",
    "                          xyticks=True,\n",
    "                          xyplotlabels=True,\n",
    "                          sum_stats=True,\n",
    "                          figsize=None,\n",
    "                          cmap='Blues',\n",
    "                          title=None,\n",
    "                          ):\n",
    "    '''\n",
    "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
    "    Arguments\n",
    "    ---------\n",
    "    cf_:            Confusion matrix to be passed in\n",
    "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n",
    "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n",
    "    count:         If True, show the raw number in the confusion matrix. Default is True.\n",
    "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n",
    "                   Default is True.\n",
    "    xyticks:       If True, show x and y ticks. Default is True.\n",
    "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n",
    "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n",
    "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n",
    "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n",
    "                   See http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                   \n",
    "    title:         Title for the heatmap. Default is None.\n",
    "    '''\n",
    "\n",
    "    cf = to_density(cf_)\n",
    "    \n",
    "    # Generate the labels for the matrix elements:\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names)==cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.2f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}\".strip() for v1, v2 in zip(group_labels,group_counts)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "\n",
    "    # Set figure paramaters:\n",
    "    if figsize==None:\n",
    "        #Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks==False:\n",
    "        #Do not show categories if xyticks is False\n",
    "        categories=False\n",
    "\n",
    "\n",
    "    # Make the heatmap:\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,yticklabels=categories_y,xticklabels=categories_x)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel(xlabel)\n",
    "        plt.xlabel(ylabel)\n",
    "\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "        \n",
    "\n",
    "# Make a confusion matrix plot for 10-fold cross-validation:\n",
    "\n",
    "make_confusion_matrix(cnf_matrix, xlabel='predicted', ylabel='true',categories_x=iris.target_names,categories_y=iris.target_names, count=True, figsize=(6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your tasks until next week:**\n",
    "\n",
    "Based on what you have seen here: Use a multiclass supervised machine learning algorithm on the three TESS feature data sets, including making diagnostic plots and the classification scores.\n",
    "\n",
    "Hint: Use the `scikit-learn` documentation (a good starting point: https://scikit-learn.org/stable/modules/multiclass.html). You can also generally search for code examples and reuse parts of the code. Reusing code is a great way to learn. As always: When reusing code, never use this without understanding what the code does!\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Processing Pipelines <a class=\"anchor\" id=\"fourth-bullet\"></a>\n",
    "\n",
    "We already have seen *data processing pipelines* to some extent:\n",
    "\n",
    "* TESS light-curve data were first detrended and outlier-cleaned: _TESS_lightcurves_raw $\\rightarrow$ _TESS_lightcurves_median_after_detrended $\\rightarrow$  _TESS_lightcurves_outliercleaned\n",
    "* features were calculated from (outlier-cleaned) TESS light-curve data\n",
    "* features were used for classification\n",
    "\n",
    "**Question:** \n",
    "Can you think of ways to improve the classification process?\n",
    "What does \"improve\" exactly mean? What we want to achieve?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large existing and upcoming surveys, such as LSST, rely and will rely heavily on data processing pipelines, e.g. for LSST:\n",
    "    \n",
    "    \n",
    "https://www.lsst.org/about/dm/pipelines\n",
    "\n",
    "https://antares.noirlab.edu/pipeline\n",
    "    \n",
    "Such surveys have the goal to:\n",
    "\n",
    "* find \"unknowns\", \"unknown unknowns\"\n",
    "* find more examples of the same types to build catalogs.\n",
    "    \n",
    "**Question:**\n",
    "How is this related to the science you are doing currently, and/or are planning to do?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary <a class=\"anchor\" id=\"fifth-bullet\"></a>\n",
    "\n",
    "At this point, all of you should have:\n",
    "\n",
    "\n",
    "* seen how `scikit-learn` works in general\n",
    "* seen some complete examples of machine learning for both unsupervised and supervised classification in the case of binary and multiclass classification\n",
    "* seen ways on how to verify machine learning results for both unsupervised and supervised classification in the case of binary and multiclass classification.\n",
    "* seen how machine learning, and data processing pipelines in general, fit into the larger picture in processing astronomical data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "7c6734141a65471e905a2515875e1c91e41dfb1a2f4fe6828aa977f658d2585b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
